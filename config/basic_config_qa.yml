base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

datasets:
  - path: data/squad_for_llms.parquet
    type:
      system_prompt: "Read the following context and concisely answer the question"
      field_system: system
      field_instruction: question
      field_input: context
      field_output: output
      format: "<|user|>\n {input} {instruction} </s>\n <|assistant|>"
      no_input_format: "<|user|>\n {instruction} </s>\n <|assistant|>"

output_dir: ./outputs/TinyLlama-1.1B-Chat-v1.0_qa

sequence_len: 8192
bf16: auto
tf32: false

adapter: lora
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

gradient_accumulation_steps: 4
micro_batch_size: 4
num_epochs: 4
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

gradient_checkpointing: true

logging_steps: 1
load_in_8bit: false